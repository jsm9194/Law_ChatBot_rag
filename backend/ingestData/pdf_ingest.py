# pdf_ingest.py
import pdfplumber
import re
import os
import json
from typing import List, Dict, Any, Tuple, Optional

# --------------------------
# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
# --------------------------
def clean_text(text: str) -> str:
    text = re.sub(r"ë²•ì œì²˜\s+.*êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„°", "", text)  # ë¨¸ë¦¬ê¸€/ê¼¬ë¦¬ê¸€ ì œê±°
    text = re.sub(
        r"\n\s*(ì‚°ì—…ì•ˆì „ë³´ê±´ê¸°ì¤€ì— ê´€í•œ ê·œì¹™|ì‚°ì—…ì•ˆì „ë³´ê±´ë²• ì‹œí–‰ê·œì¹™|ì¬ë‚œ ë° ì•ˆì „ê´€ë¦¬ ê¸°ë³¸ë²• ì‹œí–‰ë ¹|ì¬ë‚œ ë° ì•ˆì „ê´€ë¦¬ ê¸°ë³¸ë²•|ì¤‘ëŒ€ì¬í•´ ì²˜ë²Œ ë“±ì— ê´€í•œ ë²•ë¥  ì‹œí–‰ë ¹|ì¤‘ëŒ€ì¬í•´ ì²˜ë²Œ ë“±ì— ê´€í•œ ë²•ë¥ )\s*\n",
        "\n",
        text,
    )  # ë¬¸ì„œëª… ì œê±°
    text = re.sub(r"\s+\d+\s+", " ", text)  # í˜ì´ì§€ ë²ˆí˜¸ ì œê±°
    text = re.sub(r"\n{2,}", "\n", text)  # ì—°ì† ê°œí–‰ ì •ë¦¬
    return text.strip()

def normalize_structure(text: str) -> str:
    # í•­
    text = re.sub(r"\n*([â‘ -â‘³])", r"\n\1", text)
    # í˜¸
    text = re.sub(r"\n*(\d+\.\s)", r"\n\1", text)
    # ëª©
    text = re.sub(r"\n*([ê°€-í•˜]\.\s)", r"\n\1", text)
    return text

def normalize_whitespace(text: str) -> str:
    text = re.sub(r"\n+", " ", text)   # ì—¬ëŸ¬ ì¤„ë°”ê¿ˆ â†’ ê³µë°±
    text = re.sub(r"\s{2,}", " ", text)  # ì—°ì† ê³µë°± â†’ í•˜ë‚˜
    # ì˜ë¦° ë‹¨ì–´ ë³´ì •
    text = re.sub(r"(\w)\s+ë‹¤\b", r"\1ë‹¤", text)
    text = re.sub(r"í•˜ ì—¬ì•¼", "í•˜ì—¬ì•¼", text)
    text = re.sub(r"ë˜ ì§€", "ë˜ì§€", text)
    text = re.sub(r"ì•Š ëŠ”", "ì•ŠëŠ”", text)
    text = re.sub(r"ì•Š ë„ë¡", "ì•Šë„ë¡", text)
    return text.strip()

# --------------------------
# <...>, [...] ì£¼ì„ ì¶”ì¶œ
# --------------------------
def extract_annotations(text: str):
    annotations = []
    annotations += re.findall(r"<([^>]+)>", text)
    text = re.sub(r"<[^>]+>", "", text)
    annotations += re.findall(r"\[([^\]]+)\]", text)
    text = re.sub(r"\[[^\]]+\]", "", text)
    return [a.strip() for a in annotations], text.strip()

# --------------------------
# ë²•ë ¹ëª… ì¶”ì¶œ
# --------------------------
def extract_law_name(text: str):
    m = re.search(r"^(.*?ê·œì¹™|.*?ë²•)", text)
    return m.group(1).strip() if m else None

# --------------------------
# í•­/í˜¸/ëª© ë¶„ë¦¬
# --------------------------
CIRCLED_NUM_MAP = {
    "â‘ ": "1","â‘¡": "2","â‘¢": "3","â‘£": "4","â‘¤": "5",
    "â‘¥": "6","â‘¦": "7","â‘§": "8","â‘¨": "9","â‘©": "10",
    "â‘ª": "11","â‘«": "12","â‘¬": "13","â‘­": "14","â‘®": "15",
    "â‘¯": "16","â‘°": "17","â‘±": "18","â‘²": "19","â‘³": "20"
}

def split_subitems(item_text: str):
    valid_subletters = set("ê°€ë‚˜ë‹¤ë¼ë§ˆë°”ì‚¬ì•„ìì°¨ì¹´íƒ€íŒŒí•˜")
    parts = re.split(r"(?:^|\s)([ê°€-í•˜])\.\s", item_text)
    subitems, current = [], None
    for part in parts:
        if re.match(r"^[ê°€-í•˜]$", part) and part in valid_subletters:
            if current:
                current["text"] = normalize_structure(current["text"])
                subitems.append(current)
            current = {"subitem_number": part, "text": ""}
        else:
            if current:
                current["text"] += part.strip() + " "
    if current:
        current["text"] = normalize_structure(current["text"])
        subitems.append(current)
    return subitems if subitems else None

def split_items(paragraph_text: str):
    annotations, clean_txt = extract_annotations(paragraph_text)
    pattern = r"(?:^|\s)([1-9][0-9]?)\.\s"
    if not re.search(pattern, clean_txt):
        return None, clean_txt.strip(), annotations
    parts = re.split(pattern, clean_txt)
    items, current = [], None
    intro_text = ""
    for idx, part in enumerate(parts):
        if re.match(r"^[1-9][0-9]?$", part):
            if current:
                current["text"] = normalize_structure(current["text"])
                subitems = split_subitems(current["text"])
                if subitems:
                    intro_match = re.split(r"\s[ê°€-í•˜]\.\s", current["text"], maxsplit=1)
                    if intro_match:
                        current["text"] = intro_match[0].strip()
                    current["subitems"] = subitems
                items.append(current)
            current = {"item_number": part, "text": ""}
        else:
            if idx == 0:
                intro_text = normalize_structure(part)
            else:
                if current:
                    current["text"] += part.strip() + " "
    if current:
        current["text"] = normalize_structure(current["text"])
        subitems = split_subitems(current["text"])
        if subitems:
            intro_match = re.split(r"\s[ê°€-í•˜]\.\s", current["text"], maxsplit=1)
            if intro_match:
                current["text"] = intro_match[0].strip()
            current["subitems"] = subitems
        items.append(current)
    return (items if items else None, intro_text.strip(), annotations)

def split_paragraphs(article_text: str):
    parts = re.split(r"(?:\n)?([â‘ -â‘³])", article_text)
    paragraphs, current = [], None
    for part in parts:
        if not part.strip():
            continue
        if re.match(r"[â‘ -â‘³]", part):
            if current:
                current["text"] = normalize_structure(current["text"])
                items, intro_text, annotations = split_items(current["text"])
                if items:
                    current["items"] = items
                    current["text"] = intro_text
                    if annotations:
                        current["annotations"] = annotations
                paragraphs.append(current)
            num = CIRCLED_NUM_MAP.get(part, part)
            current = {"paragraph_number": num, "text": ""}
        else:
            if current:
                current["text"] += part.strip() + " "
            else:
                current = {"paragraph_number": "ë³¸ë¬¸", "text": part.strip() + " "}
    if current:
        current["text"] = normalize_structure(current["text"])
        items, intro_text, annotations = split_items(current["text"])
        if items:
            current["items"] = items
            current["text"] = intro_text
            if annotations:
                current["annotations"] = annotations
        paragraphs.append(current)
    return paragraphs

# --------------------------
# bbox ê³„ì‚°
# --------------------------
def get_bbox_for_text(words, target_text):
    if not words:
        return None
    x0 = min(w["x0"] for w in words)
    top = min(w["top"] for w in words)
    x1 = max(w["x1"] for w in words)
    bottom = max(w["bottom"] for w in words)
    return [x0, top, x1, bottom]

# --------------------------
# ì¡°ë¬¸ ë‹¨ìœ„ ë¶„ë¦¬
# --------------------------
def chunk_by_articles(full_text: str, law_name: str, page_map: dict, bbox_map: dict):
    pattern = r"(?=ì œ\s*\d+ì¡°(?:ì˜\d+)?\s*\(.+?\))"
    raw_articles = re.split(pattern, full_text)
    chunks = []
    current_chapter, current_section, current_subsection = None, None, None
    for art in raw_articles:
        art = art.strip()
        if not art:
            continue
        chap = re.search(r"(ì œ\s*\d+í¸\s*.+)", art)
        sec = re.search(r"(ì œ\s*\d+ì¥\s*.+)", art)
        subsec = re.search(r"(ì œ\s*\d+ê´€\s*.+)", art)
        if chap:
            current_chapter = chap.group(1).strip()
            art = re.sub(r"ì œ\s*\d+í¸\s*.+", "", art)
        if sec:
            current_section = sec.group(1).strip()
            art = re.sub(r"ì œ\s*\d+ì¥\s*.+", "", art)
        if subsec:
            current_subsection = subsec.group(1).strip()
            art = re.sub(r"ì œ\s*\d+ê´€\s*.+", "", art)
        m = re.match(r"(ì œ\d+ì¡°(?:ì˜\d+)?)\s*\((.+?)\)", art)
        if not m:
            continue
        article_number = m.group(1)
        article_title = m.group(2)
        body = art[m.end():].strip()
        if body and len(body) > 10:
            chunks.append({
                "type": "article",
                "article_number": article_number,
                "article_title": article_title,
                "chapter": current_chapter,
                "section": current_section,
                "subsection": current_subsection,
                "paragraphs": split_paragraphs(body),
                "law_name": law_name,
                "page_number": page_map.get(article_number),
                "bbox": bbox_map.get(article_number)
            })
    return chunks

# --------------------------
# [ADDED] Annex(ë³„í‘œ/ë¶€ì¹™/ë³„ì§€) ì²˜ë¦¬ ìœ í‹¸
# ============================================================

# ë³„í‘œ í—¤ë”: ë¼ì¸ì˜ ì–´ëŠ ìœ„ì¹˜ë“  [ë³„í‘œ N] í˜•íƒœê°€ ë“±ì¥ (ì•ì— â– , ë¬¸ì„œëª… ë“±ì´ ë¶™ì„ ìˆ˜ ìˆìŒ)
ANNEX_HEADER_REGEX = re.compile(
    r"(?m)^\s*(?:â– \s*)?.{0,100}?\[ë³„í‘œ\s*(?P<num>\d+)\]\s*(?P<header_rest>[^\n]*)"
)

# '... ê´€ë ¨'ì—ì„œ ì¡°ë¬¸ ì—°ê²°
RELATED_ARTICLE_REGEX = re.compile(r"(ì œ\d+ì¡°(?:ì œ\d+í•­)?)\s*ê´€ë ¨")

def clean_cell(v):
    if v is None:
        return ""
    return re.sub(r"\s+", " ", str(v)).strip()

def normalize_header_row(row: List[Any]) -> List[str]:
    headers = [clean_cell(c) for c in row]
    for i, h in enumerate(headers):
        if not h:
            headers[i] = f"col_{i+1}"
    return headers

def table_to_records(table: List[List[Any]]) -> List[Dict[str, Any]]:
    """
    pdfplumber.extract_tables()ì—ì„œ ì–»ì€ ë‹¨ì¼ í…Œì´ë¸”(list[list])ì„
    dict ë ˆì½”ë“œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    """
    if not table or not any(row for row in table):
        return []
    header = normalize_header_row(table[0])
    records = []
    for r in table[1:]:
        row = [clean_cell(c) for c in r]
        if len(row) < len(header):
            row = row + [""] * (len(header) - len(row))
        elif len(row) > len(header):
            row = row[:len(header)]
        rec = {header[i]: row[i] for i in range(len(header))}
        records.append(rec)
    return records

def records_to_embed_text_rows(records: List[Dict[str, Any]], annex_meta: Dict[str, Any]) -> List[str]:
    """
    í–‰ ë‹¨ìœ„ ì„ë² ë”©ìš© ì§ë ¬ ë¬¸ìì—´ ìƒì„±
    ex) "ìœ í•´ì¸ì=ë²¤ì   | TWA(ppm)=0.5 | ... | ê·¼ê±°=ë³„í‘œ 19 / ì œ145ì¡°ì œ1í•­ / ìœ í•´ì¸ìë³„ ë…¸ì¶œ ë†ë„ì˜ í—ˆìš©ê¸°ì¤€"
    """
    out = []
    basis = []
    if annex_meta.get("annex_number"):
        basis.append(annex_meta["annex_number"])
    if annex_meta.get("related_article"):
        basis.append(annex_meta["related_article"])
    if annex_meta.get("title"):
        basis.append(annex_meta["title"])
    basis_str = " / ".join(basis) if basis else ""
    for rec in records:
        pairs = [f"{k}={v}" for k, v in rec.items() if str(v).strip() != ""]
        line = " | ".join(pairs)
        if basis_str:
            line = f"{line} | ê·¼ê±°={basis_str}"
        out.append(line)
    return out

def build_page_spans(page_texts: List[str]) -> List[Tuple[int, int]]:
    """
    ê° í˜ì´ì§€ í…ìŠ¤íŠ¸ ê¸¸ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ full_textì—ì„œì˜ [start, end) spanì„ ê³„ì‚°.
    return: [(start0, end0), (start1, end1), ...]
    """
    spans = []
    cur = 0
    for t in page_texts:
        start = cur
        end = cur + len(t) + 1  # í˜ì´ì§€ ê°„ ê°œí–‰ í¬í•¨
        spans.append((start, end))
        cur = end
    return spans

def index_to_page(page_spans: List[Tuple[int, int]], idx: int) -> int:
    """
    full_text ì¸ë±ìŠ¤ â†’ í˜ì´ì§€ ë²ˆí˜¸(1-based) ì¶”ì •
    """
    for i, (s, e) in enumerate(page_spans):
        if s <= idx < e:
            return i + 1
    return len(page_spans)  # ë§ë¯¸ë©´ ë§ˆì§€ë§‰ í˜ì´ì§€ë¡œ

def span_to_pages(page_spans: List[Tuple[int, int]], start_idx: int, end_idx: int) -> List[int]:
    """
    êµ¬ê°„[start_idx, end_idx) ì´ ê±¸ì¹˜ëŠ” í˜ì´ì§€ ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸(1-based)
    """
    pages = set()
    for i, (s, e) in enumerate(page_spans):
        if not (end_idx <= s or e <= start_idx):
            pages.add(i + 1)
    return sorted(pages)

def extract_annex_blocks(full_text: str) -> List[Dict[str, Any]]:
    """
    full_textì—ì„œ [ë³„í‘œ N] í—¤ë”ë“¤ì„ ì°¾ì•„ ê° ë¸”ë¡ì˜ ì‹œì‘/ë ì¸ë±ìŠ¤ë¥¼ ì‚°ì •
    """
    matches = list(ANNEX_HEADER_REGEX.finditer(full_text))
    annexes = []
    for i, m in enumerate(matches):
        start = m.start()
        end = matches[i + 1].start() if i + 1 < len(matches) else len(full_text)
        annex_number = f"ë³„í‘œ {m.group('num')}"
        header_rest = m.group("header_rest").strip()
        header_text = full_text[start: full_text.find("\n", start) if "\n" in full_text[start:] else end]
        annexes.append({
            "start_idx": start,
            "end_idx": end,
            "annex_number": annex_number,
            "header_rest": header_rest,
            "header_text": header_text
        })
    return annexes

def annex_title_and_related(annex_text: str, header_rest: str) -> Tuple[Optional[str], Optional[str]]:
    """
    ë³„í‘œ ì œëª©/ê´€ë ¨ ì¡°ë¬¸ ì¶”ì¶œ
    """
    # ì œëª© í›„ë³´: í—¤ë” ë¼ì¸ ë°”ë¡œ ë’¤ ì²« ë¹„ì–´ìˆì§€ ì•Šì€ ì¤„ ë˜ëŠ” header_rest
    lines = [ln.strip() for ln in annex_text.splitlines() if ln.strip()]
    title = header_rest if header_rest else (lines[1] if len(lines) > 1 else (lines[0] if lines else None))
    # '... (ì œ145ì¡°ì œ1í•­ ê´€ë ¨)' ê°™ì€ ë¬¸êµ¬ì—ì„œ ê´€ë ¨ ì¡°ë¬¸
    rel = None
    m = RELATED_ARTICLE_REGEX.search(annex_text)
    if m:
        rel = m.group(1)
    return title, rel

def extract_tables_from_pages(pdf: pdfplumber.PDF, page_numbers: List[int]) -> List[List[List[Any]]]:
    """
    í•´ë‹¹ í˜ì´ì§€ë“¤ì—ì„œ í‘œ ì¶”ì¶œ ì‹œë„ (ì—¬ëŸ¬ í…Œì´ë¸” í•©ì¹¨)
    """
    all_tables: List[List[List[Any]]] = []
    for pno in page_numbers:
        try:
            page = pdf.pages[pno - 1]
            tables = page.extract_tables() or []
            for tb in tables:
                # í‘œë¡œ ì¸ì • ê°€ëŠ¥í•œ ìµœì†Œ ì¡°ê±´: í–‰ 2ì¤„ ì´ìƒ(í—¤ë”+ë°ì´í„°)
                if tb and len(tb) >= 2 and any(any(cell for cell in row) for row in tb):
                    all_tables.append(tb)
        except Exception:
            # í‘œ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ(ì•„ë˜ì—ì„œ í…ìŠ¤íŠ¸ fallback)
            continue
    return all_tables

def page_bbox(words_by_page: Dict[int, List[Dict[str, Any]]], page_numbers: List[int]) -> List[Optional[List[float]]]:
    """
    ê° í˜ì´ì§€ì˜ bbox ë¦¬ìŠ¤íŠ¸ (ì›Œë“œ ì „ì—­ ë²”ìœ„)
    """
    bboxes = []
    for pno in page_numbers:
        words = words_by_page.get(pno)
        bboxes.append(get_bbox_for_text(words, ""))  # ì „ì²´ ë²”ìœ„
    return bboxes

def parse_annexes(full_text: str,
                  page_spans: List[Tuple[int, int]],
                  pdf: pdfplumber.PDF,
                  words_by_page: Dict[int, List[Dict[str, Any]]],
                  law_name: Optional[str],
                  source_pdf: str) -> List[Dict[str, Any]]:
    """
    full_text ê¸°ë°˜ìœ¼ë¡œ ë³„í‘œ ë¸”ë¡ì„ ì¶”ì¶œí•˜ê³ , í˜ì´ì§€/í‘œ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ì—¬ annex chunk ìƒì„±
    """
    annex_blocks = extract_annex_blocks(full_text)
    annex_chunks: List[Dict[str, Any]] = []

    for blk in annex_blocks:
        start_idx, end_idx = blk["start_idx"], blk["end_idx"]
        pages = span_to_pages(page_spans, start_idx, end_idx) or [index_to_page(page_spans, start_idx)]
        text_block = full_text[start_idx:end_idx].strip()

        title, related = annex_title_and_related(text_block, blk.get("header_rest", ""))

        # í‘œ ì¶”ì¶œ ì‹œë„
        tables = extract_tables_from_pages(pdf, pages)
        table_records: List[Dict[str, Any]] = []
        if tables:
            # ì—¬ëŸ¬ í…Œì´ë¸”ì„ ìˆœì„œëŒ€ë¡œ recordsë¡œ í™•ì¥
            for tb in tables:
                recs = table_to_records(tb)
                # ë¹ˆ ë ˆì½”ë“œë§Œ ìˆëŠ” í…Œì´ë¸”ì€ ìŠ¤í‚µ
                if recs:
                    table_records.extend(recs)

        chunk: Dict[str, Any] = {
            "type": "annex",
            "annex_number": blk["annex_number"],
            "title": title,
            "related_article": related,
            "law_name": law_name,
            "source_pdf": source_pdf,
            # ë³„í‘œëŠ” ì—¬ëŸ¬ í˜ì´ì§€ë¥¼ ê±¸ì¹  ìˆ˜ ìˆìŒ
            "page_numbers": pages,
            "bboxes": page_bbox(words_by_page, pages),
        }

        if table_records:
            chunk["table"] = table_records
            # ì„ë² ë”©ìš© í–‰ ì§ë ¬ ë¬¸ìì—´(ìë™) â€” í›„ì† íŒŒì´í”„ë¼ì¸ì—ì„œ ì„ íƒ ì‚¬ìš©
            chunk["embed_text_rows"] = records_to_embed_text_rows(
                table_records,
                {"annex_number": blk["annex_number"], "title": title, "related_article": related},
            )
            # í…ìŠ¤íŠ¸ ì›ë¬¸ë„ í•¨ê»˜ ë³´ê´€(ê²€ì¦/ê²€ìƒ‰ìš©)
            chunk["content"] = text_block
        else:
            # í‘œê°€ ì¶”ì¶œë˜ì§€ ì•Šìœ¼ë©´ í…ìŠ¤íŠ¸ ë¸”ë¡ ê·¸ëŒ€ë¡œ ì €ì¥ (UIì—ì„œ PDF ì›ë¬¸ í™•ì¸)
            chunk["content"] = text_block

        annex_chunks.append(chunk)

    return annex_chunks


# --------------------------
# PDF â†’ JSON ë³€í™˜
# --------------------------
def pdf_to_chunks(pdf_path: str):
    full_text = ""
    page_map, bbox_map = {}, {}
    page_texts: List[str] = []
    words_by_page: Dict[int, List[Dict[str, Any]]] = {}

    with pdfplumber.open(pdf_path) as pdf:
        for page_number, page in enumerate(pdf.pages, start=1):
            text = page.extract_text()
            if not text:
                page_texts.append("")
                continue
            cleaned = clean_text(text)
            page_texts.append(cleaned)
            full_text += cleaned + "\n"

            words = page.extract_words() or []
            words_by_page[page_number] = words

            for match in re.finditer(r"(ì œ\d+ì¡°(?:ì˜\d+)?)", cleaned):
                article_num = match.group(1)
                page_map[article_num] = page_number
                bbox_map[article_num] = get_bbox_for_text(words, article_num)

        head_for_law = ""
        try:
            head_for_law = full_text.split("ì œ1í¸")[0]
        except Exception:
            head_for_law = full_text[:1000]
        law_name = extract_law_name(head_for_law)

        article_chunks = chunk_by_articles(full_text, law_name, page_map, bbox_map)
        src_pdf_name = os.path.basename(pdf_path)
        for c in article_chunks:
            c["source_pdf"] = src_pdf_name

        page_spans = build_page_spans(page_texts)
        annex_chunks = parse_annexes(
            full_text=full_text,
            page_spans=page_spans,
            pdf=pdf,
            words_by_page=words_by_page,
            law_name=law_name,
            source_pdf=src_pdf_name,
        )

    chunks = article_chunks + annex_chunks

    # [ë§ˆì§€ë§‰ ë‹¨ê³„] ëª¨ë“  í…ìŠ¤íŠ¸ì— whitespace ì •ë¦¬ ì ìš©
    for c in chunks:
        if "paragraphs" in c:
            for p in c["paragraphs"]:
                if "text" in p:
                    p["text"] = normalize_whitespace(p["text"])
                if "items" in p:
                    for item in p["items"]:
                        if "text" in item:
                            item["text"] = normalize_whitespace(item["text"])
                        if "subitems" in item:
                            for sub in item["subitems"]:
                                if "text" in sub:
                                    sub["text"] = normalize_whitespace(sub["text"])
        if "content" in c:
            c["content"] = normalize_whitespace(c["content"])

    return chunks

# --------------------------
# ì‹¤í–‰
# --------------------------
if __name__ == "__main__":
    pdf_folder = "pdfs"
    output_folder = "texts"
    os.makedirs(output_folder, exist_ok=True)

    for pdf_file in os.listdir(pdf_folder):
        if not pdf_file.lower().endswith(".pdf"):
            continue

        pdf_path = os.path.join(pdf_folder, pdf_file)
        try:
            chunks = pdf_to_chunks(pdf_path)
            output_file = os.path.join(output_folder, pdf_file.replace(".pdf", ".json"))
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(chunks, f, ensure_ascii=False, indent=2)
            print(f"ğŸ“– {pdf_file} â†’ {len(chunks)}ê°œ ì²­í¬ ì €ì¥ ì™„ë£Œ â†’ {output_file}")
        except Exception as e:
            print(f"âš ï¸ {pdf_file} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
