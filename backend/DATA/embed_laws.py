import os
import json
import hashlib
from dotenv import load_dotenv
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct, VectorParams
from openai import OpenAI
from tqdm import tqdm

# --------------------------
# í™˜ê²½ì„¤ì •
# --------------------------
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

qdrant = QdrantClient(host="localhost", port=6333)

COLLECTION_NAME = "laws"
DIM = 3072  # text-embedding-3-large ì°¨ì› ìˆ˜

# --------------------------
# Qdrant ì»¬ë ‰ì…˜ ì´ˆê¸°í™”
# --------------------------
try:
    qdrant.delete_collection(COLLECTION_NAME)
    print(f"ğŸ—‘ ê¸°ì¡´ ì»¬ë ‰ì…˜ {COLLECTION_NAME} ì‚­ì œ ì™„ë£Œ")
except Exception:
    print("âš ï¸ ê¸°ì¡´ ì»¬ë ‰ì…˜ ì—†ìŒ, ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤")

qdrant.create_collection(
    collection_name=COLLECTION_NAME,
    vectors_config=VectorParams(size=DIM, distance="Cosine"),
)
print(f"âœ… ìƒˆ ì»¬ë ‰ì…˜ {COLLECTION_NAME} ìƒì„± ì™„ë£Œ")

# --------------------------
# ìœ í‹¸ í•¨ìˆ˜
# --------------------------
def hash_id(law_name: str, article_key: str, text: str) -> str:
    """ë²•ë ¹ëª… + ì¡°ë¬¸í‚¤ + í…ìŠ¤íŠ¸ë¥¼ í•´ì‹œí•´ì„œ point_idë¡œ ì‚¬ìš©"""
    raw = law_name + article_key + text
    return hashlib.md5(raw.encode("utf-8")).hexdigest()

def get_embeddings(texts: list[str]) -> list[list[float]]:
    """OpenAI APIë¡œ ë°°ì¹˜ ì„ë² ë”© ìƒì„±"""
    response = client.embeddings.create(
        model="text-embedding-3-large",
        input=texts
    )
    return [d.embedding for d in response.data]

# --------------------------
# ë©”ì¸ ë¡œì§
# --------------------------
INPUT_DIR = "./ChunkedData"

def main():
    for fname in os.listdir(INPUT_DIR):
        if not fname.endswith("_chunked.json"):
            continue

        with open(os.path.join(INPUT_DIR, fname), "r", encoding="utf-8") as f:
            data = json.load(f)

        articles = data["ë²•ë ¹"]["ì¡°ë¬¸"].get("ì¡°ë¬¸ë‹¨ìœ„", [])

        for article in tqdm(articles, desc=f"{fname} ì—…ë¡œë“œ ì¤‘"):
            if "embedding_chunks" not in article:
                continue

            law_name = article.get("law_name", fname.replace("_chunked.json", ""))
            article_key = article.get("ì¡°ë¬¸í‚¤", "")
            article_number = article.get("ì¡°ë¬¸ë²ˆí˜¸", "")
            article_title = article.get("ì¡°ë¬¸ì œëª©", "")

            # âœ… ë¹ˆ í…ìŠ¤íŠ¸ ì œê±°
            chunks = [c for c in article["embedding_chunks"] if c.strip()]
            if not chunks:
                continue

            # âœ… ë°°ì¹˜ë¡œ ì„ë² ë”© ìƒì„±
            embeddings = get_embeddings(chunks)

            points = []
            for chunk, emb in zip(chunks, embeddings):
                # âœ… ë²¡í„° ê¸¸ì´ ê²€ì¦
                if len(emb) != DIM:
                    print(f"âŒ ì˜ëª»ëœ ë²¡í„° ê¸¸ì´ {len(emb)} for chunk: {chunk[:50]}")
                    continue

                point_id = hash_id(law_name, article_key, chunk)

                payload = {
                    "law_name": law_name,
                    "article_number": article_number,
                    "article_title": article_title,
                    "article_key": article_key,
                    "text": chunk,
                    "amendments": article.get("amendments", []),
                    "all_change_dates": article.get("all_change_dates", []),
                }

                points.append(PointStruct(id=point_id, vector=emb, payload=payload))

            # âœ… upsert ì‹¤í–‰
            if points:
                qdrant.upsert(collection_name=COLLECTION_NAME, points=points)

        print(f"âœ… {fname} ì„ë² ë”© ë° Qdrant ì ì¬ ì™„ë£Œ")

if __name__ == "__main__":
    main()
