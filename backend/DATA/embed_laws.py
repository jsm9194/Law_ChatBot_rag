import os
import json
import hashlib
import time
from dotenv import load_dotenv
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct, VectorParams
from openai import OpenAI
from tqdm import tqdm

# --------------------------
# í™˜ê²½ì„¤ì •
# --------------------------
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

qdrant = QdrantClient(host="localhost", port=6333)

COLLECTION_NAME = "laws"
DIM = 3072  # text-embedding-3-large ì°¨ì› ìˆ˜
BATCH_SIZE = 100  # ì„ë² ë”© ë°°ì¹˜ í¬ê¸°
MAX_RETRIES = 3   # ì¬ì‹œë„ íšŸìˆ˜
INPUT_DIR = "./ChunkedData"

# --------------------------
# Qdrant ì»¬ë ‰ì…˜ ì´ˆê¸°í™”
# --------------------------
try:
    qdrant.delete_collection(COLLECTION_NAME)
    print(f"ğŸ—‘ ê¸°ì¡´ ì»¬ë ‰ì…˜ {COLLECTION_NAME} ì‚­ì œ ì™„ë£Œ")
except Exception:
    print("âš ï¸ ê¸°ì¡´ ì»¬ë ‰ì…˜ ì—†ìŒ, ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤")

qdrant.create_collection(
    collection_name=COLLECTION_NAME,
    vectors_config=VectorParams(size=DIM, distance="Cosine"),
)
print(f"âœ… ìƒˆ ì»¬ë ‰ì…˜ {COLLECTION_NAME} ìƒì„± ì™„ë£Œ")

# --------------------------
# ìœ í‹¸ í•¨ìˆ˜
# --------------------------
def hash_id(*args) -> str:
    """ì—¬ëŸ¬ ì‹ë³„ ì •ë³´ë¥¼ í•©ì³ í•´ì‹œ ìƒì„±"""
    raw = "_".join(str(a) for a in args if a)
    return hashlib.md5(raw.encode("utf-8")).hexdigest()


def get_embeddings_with_retry(texts: list[str]) -> list[list[float]]:
    """OpenAI API ì„ë² ë”© ìš”ì²­ (ì¬ì‹œë„ í¬í•¨)"""
    for attempt in range(MAX_RETRIES):
        try:
            response = client.embeddings.create(
                model="text-embedding-3-large",
                input=texts
            )
            embeddings = [d.embedding for d in response.data]
            return embeddings
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ìš”ì²­ ì‹¤íŒ¨ (ì‹œë„ {attempt+1}/{MAX_RETRIES}): {e}")
            time.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
    raise RuntimeError("ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ (ì¬ì‹œë„ ì´ˆê³¼)")


def extract_all_chunks(article: dict) -> list[str]:
    """
    ì¡°ë¬¸ ë° í•˜ìœ„ í•­ëª©(í•­, í˜¸, ëª© ë“±)ì˜ embedding_chunksë¥¼ ì¬ê·€ì ìœ¼ë¡œ ì¶”ì¶œ
    ë‹¤ì–‘í•œ JSON êµ¬ì¡°(ë¦¬ìŠ¤íŠ¸/ë¬¸ìì—´ í˜¼í•©)ë¥¼ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
    """
    chunks = []

    # í˜„ì¬ ë ˆë²¨ì˜ embedding_chunks
    if "embedding_chunks" in article:
        chunks.extend([c for c in article["embedding_chunks"] if c.strip()])

    # í•˜ìœ„ êµ¬ì¡° ì¬ê·€ íƒìƒ‰
    for key in ["í•­", "í˜¸", "ëª©"]:
        if key not in article:
            continue

        sub_item = article[key]

        # 1ï¸âƒ£ ë¦¬ìŠ¤íŠ¸ì¼ ê²½ìš°
        if isinstance(sub_item, list):
            for sub in sub_item:
                if isinstance(sub, dict):
                    chunks.extend(extract_all_chunks(sub))

        # 2ï¸âƒ£ ë”•ì…”ë„ˆë¦¬ì¼ ê²½ìš°
        elif isinstance(sub_item, dict):
            chunks.extend(extract_all_chunks(sub_item))

        # 3ï¸âƒ£ ë¬¸ìì—´(ë‹¨ìˆœ í…ìŠ¤íŠ¸)ì¼ ê²½ìš°
        elif isinstance(sub_item, str):
            if sub_item.strip():
                chunks.append(sub_item.strip())

    return chunks


def build_payload(article: dict, chunk: str, law_name: str) -> dict:
    """Qdrantì— ì €ì¥í•  payload êµ¬ì„±"""
    return {
        "law_name": law_name,
        "ì¡°ë¬¸ë²ˆí˜¸": article.get("ì¡°ë¬¸ë²ˆí˜¸", ""),
        "í•­ë²ˆí˜¸": article.get("í•­ë²ˆí˜¸", ""),
        "í˜¸ë²ˆí˜¸": article.get("í˜¸ë²ˆí˜¸", ""),
        "ëª©ë²ˆí˜¸": article.get("ëª©ë²ˆí˜¸", ""),
        "ì¡°ë¬¸ì œëª©": article.get("ì¡°ë¬¸ì œëª©", ""),
        "ì¡°ë¬¸ë‚´ìš©": chunk,
        "ì¡°ë¬¸í‚¤": article.get("ì¡°ë¬¸í‚¤", ""),
        "ì¡°ë¬¸ì‹œí–‰ì¼ì": article.get("ì¡°ë¬¸ì‹œí–‰ì¼ì", ""),
        "amendments": article.get("amendments", []),
        "all_change_dates": article.get("all_change_dates", []),
    }


# --------------------------
# ë©”ì¸ ë¡œì§
# --------------------------
def main():
    for fname in os.listdir(INPUT_DIR):
        if not fname.endswith("_chunked.json"):
            continue

        with open(os.path.join(INPUT_DIR, fname), "r", encoding="utf-8") as f:
            data = json.load(f)

        articles = data["ë²•ë ¹"]["ì¡°ë¬¸"].get("ì¡°ë¬¸ë‹¨ìœ„", [])
        total_points = 0

        for article in tqdm(articles, desc=f"{fname} ì—…ë¡œë“œ ì¤‘"):
            # âœ… ì¡°ë¬¸ ë° í•˜ìœ„ í•­ëª© ì „ì²´ì—ì„œ ì²­í¬ ì¶”ì¶œ
            all_chunks = extract_all_chunks(article)
            if not all_chunks:
                continue

            law_name = article.get("law_name", fname.replace("_chunked.json", ""))
            article_key = article.get("ì¡°ë¬¸í‚¤", "")

            # âœ… ë°°ì¹˜ ë‹¨ìœ„ ì„ë² ë”© ìƒì„± ë° ì—…ë¡œë“œ
            for i in range(0, len(all_chunks), BATCH_SIZE):
                batch = all_chunks[i:i + BATCH_SIZE]
                embeddings = get_embeddings_with_retry(batch)

                points = []
                for chunk, emb in zip(batch, embeddings):
                    if len(emb) != DIM:
                        raise ValueError(
                            f"âŒ ë²¡í„° ì°¨ì› ë¶ˆì¼ì¹˜: expected {DIM}, got {len(emb)} "
                            f"(í…ìŠ¤íŠ¸ ì•ë¶€ë¶„: {chunk[:50]!r})"
                        )

                    point_id = hash_id(law_name, article_key, chunk)
                    payload = build_payload(article, chunk, law_name)
                    points.append(PointStruct(id=point_id, vector=emb, payload=payload))

                if points:
                    qdrant.upsert(collection_name=COLLECTION_NAME, points=points)
                    total_points += len(points)

        print(f"âœ… {fname} ì„ë² ë”© ë° Qdrant ì ì¬ ì™„ë£Œ ({total_points}ê°œ ë²¡í„°)")

if __name__ == "__main__":
    main()
