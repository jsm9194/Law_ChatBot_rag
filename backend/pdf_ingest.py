# pdf_ingest.py
import pdfplumber
import re
import os
import json

# --------------------------
# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
# --------------------------
def clean_text(text: str) -> str:
    text = re.sub(r"ë²•ì œì²˜\s+.*êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„°", "", text)  # ë¨¸ë¦¬ê¸€/ê¼¬ë¦¬ê¸€ ì œê±°
    text = re.sub(r"\n\s*(ì‚°ì—…ì•ˆì „ë³´ê±´ê¸°ì¤€ì— ê´€í•œ ê·œì¹™|ì‚°ì—…ì•ˆì „ë³´ê±´ë²• ì‹œí–‰ê·œì¹™|ì¬ë‚œ ë° ì•ˆì „ê´€ë¦¬ ê¸°ë³¸ë²• ì‹œí–‰ë ¹|ì¬ë‚œ ë° ì•ˆì „ê´€ë¦¬ ê¸°ë³¸ë²•|ì¤‘ëŒ€ì¬í•´ ì²˜ë²Œ ë“±ì— ê´€í•œ ë²•ë¥  ì‹œí–‰ë ¹|ì¤‘ëŒ€ì¬í•´ ì²˜ë²Œ ë“±ì— ê´€í•œ ë²•ë¥ )\s*\n", "\n", text)  # ë¬¸ì„œëª… ì œê±°
    text = re.sub(r"\s+\d+\s+", " ", text)  # í˜ì´ì§€ ë²ˆí˜¸ ì œê±°
    text = re.sub(r"\n{2,}", "\n", text)  # ì—°ì† ê°œí–‰ ì •ë¦¬
    return text.strip()

def normalize_whitespace(text: str) -> str:
    text = re.sub(r"\n*([â‘ -â‘³])", r"\n\1", text)  # í•­
    text = re.sub(r"\n*(\d+\.\s)", r"\n\1", text)  # í˜¸
    text = re.sub(r"\n*([ê°€-í•˜]\.\s)", r"\n\1", text)  # ëª©
    text = re.sub(r"\n+", " ", text)
    text = re.sub(r"\s{2,}", " ", text)
    # ì˜ë¦° ë‹¨ì–´ ë³´ì •
    text = re.sub(r"(\w)\s+ë‹¤\b", r"\1ë‹¤", text)
    text = re.sub(r"í•˜ ì—¬ì•¼", "í•˜ì—¬ì•¼", text)
    text = re.sub(r"ë˜ ì§€", "ë˜ì§€", text)
    text = re.sub(r"ì•Š ëŠ”", "ì•ŠëŠ”", text)
    text = re.sub(r"ì•Š ë„ë¡", "ì•Šë„ë¡", text)
    return text.strip()

# --------------------------
# <...>, [...] ì£¼ì„ ì¶”ì¶œ
# --------------------------
def extract_annotations(text: str):
    annotations = []
    annotations += re.findall(r"<([^>]+)>", text)
    text = re.sub(r"<[^>]+>", "", text)
    annotations += re.findall(r"\[([^\]]+)\]", text)
    text = re.sub(r"\[[^\]]+\]", "", text)
    return [a.strip() for a in annotations], text.strip()

# --------------------------
# ë²•ë ¹ëª… ì¶”ì¶œ
# --------------------------
def extract_law_name(text: str):
    m = re.search(r"^(.*?ê·œì¹™|.*?ë²•)", text)
    return m.group(1).strip() if m else None

# --------------------------
# í•­/í˜¸/ëª© ë¶„ë¦¬
# --------------------------
CIRCLED_NUM_MAP = {
    "â‘ ": "1","â‘¡": "2","â‘¢": "3","â‘£": "4","â‘¤": "5",
    "â‘¥": "6","â‘¦": "7","â‘§": "8","â‘¨": "9","â‘©": "10",
    "â‘ª": "11","â‘«": "12","â‘¬": "13","â‘­": "14","â‘®": "15",
    "â‘¯": "16","â‘°": "17","â‘±": "18","â‘²": "19","â‘³": "20"
}

def split_subitems(item_text: str):
    valid_subletters = set("ê°€ë‚˜ë‹¤ë¼ë§ˆë°”ì‚¬ì•„ìì°¨ì¹´íƒ€íŒŒí•˜")
    parts = re.split(r"(?:^|\s)([ê°€-í•˜])\.\s", item_text)
    subitems, current = [], None
    for part in parts:
        if re.match(r"^[ê°€-í•˜]$", part) and part in valid_subletters:
            if current:
                current["text"] = normalize_whitespace(current["text"])
                subitems.append(current)
            current = {"subitem_number": part, "text": ""}
        else:
            if current:
                current["text"] += part.strip() + " "
    if current:
        current["text"] = normalize_whitespace(current["text"])
        subitems.append(current)
    return subitems if subitems else None

def split_items(paragraph_text: str):
    annotations, clean_text = extract_annotations(paragraph_text)
    pattern = r"(?:^|\s)([1-9][0-9]?)\.\s"
    if not re.search(pattern, clean_text):
        return None, clean_text.strip(), annotations
    parts = re.split(pattern, clean_text)
    items, current = [], None
    intro_text = ""
    for idx, part in enumerate(parts):
        if re.match(r"^[1-9][0-9]?$", part):
            if current:
                current["text"] = normalize_whitespace(current["text"])
                subitems = split_subitems(current["text"])
                if subitems:
                    intro_match = re.split(r"\s[ê°€-í•˜]\.\s", current["text"], maxsplit=1)
                    if intro_match:
                        current["text"] = intro_match[0].strip()
                    current["subitems"] = subitems
                items.append(current)
            current = {"item_number": part, "text": ""}
        else:
            if idx == 0:
                intro_text = normalize_whitespace(part)
            else:
                if current:
                    current["text"] += part.strip() + " "
    if current:
        current["text"] = normalize_whitespace(current["text"])
        subitems = split_subitems(current["text"])
        if subitems:
            intro_match = re.split(r"\s[ê°€-í•˜]\.\s", current["text"], maxsplit=1)
            if intro_match:
                current["text"] = intro_match[0].strip()
            current["subitems"] = subitems
        items.append(current)
    return (items if items else None, intro_text.strip(), annotations)

def split_paragraphs(article_text: str):
    parts = re.split(r"(?:\n)?([â‘ -â‘³])", article_text)
    paragraphs, current = [], None
    for part in parts:
        if not part.strip():
            continue
        if re.match(r"[â‘ -â‘³]", part):
            if current:
                current["text"] = normalize_whitespace(current["text"])
                items, intro_text, annotations = split_items(current["text"])
                if items:
                    current["items"] = items
                    current["text"] = intro_text
                    if annotations:
                        current["annotations"] = annotations
                paragraphs.append(current)
            num = CIRCLED_NUM_MAP.get(part, part)
            current = {"paragraph_number": num, "text": ""}
        else:
            if current:
                current["text"] += part.strip() + " "
            else:
                current = {"paragraph_number": "ë³¸ë¬¸", "text": part.strip() + " "}
    if current:
        current["text"] = normalize_whitespace(current["text"])
        items, intro_text, annotations = split_items(current["text"])
        if items:
            current["items"] = items
            current["text"] = intro_text
            if annotations:
                current["annotations"] = annotations
        paragraphs.append(current)
    return paragraphs

# --------------------------
# bbox ê³„ì‚°
# --------------------------
def get_bbox_for_text(words, target_text):
    if not words:
        return None
    # ë‹¨ìˆœíˆ í˜ì´ì§€ ì „ì²´ bboxë¡œ ì¡ê¸° (ì¡°ë¬¸ ì‹œì‘ ë‹¨ì–´ ê¸°ì¤€ â†’ í™•ì¥ ê°€ëŠ¥)
    x0 = min(w["x0"] for w in words)
    top = min(w["top"] for w in words)
    x1 = max(w["x1"] for w in words)
    bottom = max(w["bottom"] for w in words)
    return [x0, top, x1, bottom]

# --------------------------
# ì¡°ë¬¸ ë‹¨ìœ„ ë¶„ë¦¬
# --------------------------
def chunk_by_articles(full_text: str, law_name: str, page_map: dict, bbox_map: dict):
    pattern = r"(?=ì œ\s*\d+ì¡°(?:ì˜\d+)?\s*\(.+?\))"
    raw_articles = re.split(pattern, full_text)
    chunks = []
    current_chapter, current_section, current_subsection = None, None, None
    for art in raw_articles:
        art = art.strip()
        if not art:
            continue
        chap = re.search(r"(ì œ\s*\d+í¸\s*.+)", art)
        sec = re.search(r"(ì œ\s*\d+ì¥\s*.+)", art)
        subsec = re.search(r"(ì œ\s*\d+ê´€\s*.+)", art)
        if chap:
            current_chapter = chap.group(1).strip()
            art = re.sub(r"ì œ\s*\d+í¸\s*.+", "", art)
        if sec:
            current_section = sec.group(1).strip()
            art = re.sub(r"ì œ\s*\d+ì¥\s*.+", "", art)
        if subsec:
            current_subsection = subsec.group(1).strip()
            art = re.sub(r"ì œ\s*\d+ê´€\s*.+", "", art)
        m = re.match(r"(ì œ\d+ì¡°(?:ì˜\d+)?)\s*\((.+?)\)", art)
        if not m:
            continue
        article_number = m.group(1)
        article_title = m.group(2)
        body = art[m.end():].strip()
        if body and len(body) > 10:
            chunks.append({
                "article_number": article_number,
                "article_title": article_title,
                "chapter": current_chapter,
                "section": current_section,
                "subsection": current_subsection,
                "paragraphs": split_paragraphs(body),
                "law_name": law_name,
                "page_number": page_map.get(article_number),
                "bbox": bbox_map.get(article_number)
            })
    return chunks

# --------------------------
# PDF â†’ JSON ë³€í™˜
# --------------------------
def pdf_to_chunks(pdf_path: str):
    full_text = ""
    page_map, bbox_map = {}, {}
    with pdfplumber.open(pdf_path) as pdf:
        for page_number, page in enumerate(pdf.pages, start=1):
            text = page.extract_text()
            if not text:
                continue
            cleaned = clean_text(text)
            full_text += cleaned + "\n"
            words = page.extract_words()
            for match in re.finditer(r"(ì œ\d+ì¡°(?:ì˜\d+)?)", cleaned):
                article_num = match.group(1)
                page_map[article_num] = page_number
                bbox_map[article_num] = get_bbox_for_text(words, article_num)
    law_name = extract_law_name(full_text.split("ì œ1í¸")[0])
    chunks = chunk_by_articles(full_text, law_name, page_map, bbox_map)
    return chunks

# --------------------------
# ì‹¤í–‰
# --------------------------
if __name__ == "__main__":
    pdf_folder = "pdfs"
    output_folder = "texts"
    os.makedirs(output_folder, exist_ok=True)
    for pdf_file in os.listdir(pdf_folder):
        if pdf_file.endswith(".pdf"):
            pdf_path = os.path.join(pdf_folder, pdf_file)
            chunks = pdf_to_chunks(pdf_path)
            output_file = os.path.join(output_folder, pdf_file.replace(".pdf", ".json"))
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(chunks, f, ensure_ascii=False, indent=2)
            print(f"ğŸ“– {pdf_file} â†’ {len(chunks)}ê°œ ì¡°ë¬¸ ì €ì¥ ì™„ë£Œ â†’ {output_file}")
